{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLWxLRpbML-e"
      },
      "outputs": [],
      "source": [
        "# This cell will be deleted as its content is being split into new, more organized cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_1",
        "outputId": "50f8c3c8-8f24-4412-c84a-5bb7f96c7384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping ltn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/logictensornetworks/LTNtorch\n",
            "  Cloning https://github.com/logictensornetworks/LTNtorch to /tmp/pip-req-build-0j68s36_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/logictensornetworks/LTNtorch /tmp/pip-req-build-0j68s36_\n",
            "  Resolved https://github.com/logictensornetworks/LTNtorch to commit d1bd98169cc2121f8cdd25ff99901e4589923c95\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from LTNtorch==1.0.2) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from LTNtorch==1.0.2) (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch==1.0.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch==1.0.2) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch==1.0.2) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch==1.0.2) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch==1.0.2) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch==1.0.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->LTNtorch==1.0.2) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->LTNtorch==1.0.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->LTNtorch==1.0.2) (3.0.3)\n",
            "Building wheels for collected packages: LTNtorch\n",
            "  Building wheel for LTNtorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for LTNtorch: filename=LTNtorch-1.0.2-py3-none-any.whl size=29525 sha256=8afc9e0cdb1a729400162993f01a27cafb20ea53cbce94aa0cf1d91168ebb07f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-evxrwbvh/wheels/da/f9/39/427ffc120e0d781e4de19bbed6a73c2e766ba29f0e5d7fb589\n",
            "Successfully built LTNtorch\n",
            "Installing collected packages: LTNtorch\n",
            "Successfully installed LTNtorch-1.0.2\n",
            "Rodando em: cpu\n",
            "Configura√ß√£o inicial e importa√ß√µes conclu√≠das.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. INSTALA√á√ÉO E IMPORTA√á√ïES ESSENCIAIS ---\n",
        "# Este bloco garante que a biblioteca LTNtorch esteja instalada na vers√£o correta\n",
        "# e importa todas as bibliotecas Python necess√°rias para o projeto.\n",
        "\n",
        "# Remove qualquer vers√£o existente do LTN para evitar conflitos.\n",
        "!pip uninstall ltn -y\n",
        "# Instala a vers√£o espec√≠fica do LTNtorch diretamente do reposit√≥rio GitHub,\n",
        "# garantindo compatibilidade e acesso √†s funcionalidades mais recentes.\n",
        "!pip install git+https://github.com/logictensornetworks/LTNtorch\n",
        "\n",
        "# Importa a biblioteca LTNtorch, fundamental para a constru√ß√£o de redes tensor l√≥gicas.\n",
        "import ltn\n",
        "# Importa a biblioteca PyTorch, a base para opera√ß√µes de tensores e redes neurais.\n",
        "import torch\n",
        "# Importa o m√≥dulo de redes neurais do PyTorch, usado para definir os modelos.\n",
        "import torch.nn as nn\n",
        "# Importa NumPy para opera√ß√µes num√©ricas, especialmente √∫til para manipula√ß√£o de arrays.\n",
        "import numpy as np\n",
        "# Importa m√©tricas do scikit-learn para avalia√ß√£o de modelos, como acur√°cia e F1-score.\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# --- CONFIGURA√á√ÉO GLOBAL DO DISPOSITIVO ---\n",
        "# Configura o dispositivo (GPU CUDA ou CPU) a ser usado para todas as opera√ß√µes de tensores.\n",
        "# Prioriza a GPU se dispon√≠vel, caso contr√°rio, utiliza a CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Rodando em: {device}\")\n",
        "\n",
        "print(\"Configura√ß√£o inicial e importa√ß√µes conclu√≠das.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_2",
        "outputId": "713df351-36f2-45b8-a57f-9071be69a901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelos neurais e classe de gera√ß√£o de dados definidos.\n"
          ]
        }
      ],
      "source": [
        "# --- 2. GERA√á√ÉO DE DADOS E DEFINI√á√ÉO DOS MODELOS NEURAIS ---\n",
        "# Este bloco cont√©m a l√≥gica para gerar dados sint√©ticos (ambiente ClevrSimplified)\n",
        "# e as defini√ß√µes das classes de modelos neurais que servem como predicados no LTN.\n",
        "\n",
        "# --- GERA√á√ÉO DE DADOS (CLASSE CLEVR SIMPLIFIED) ---\n",
        "# A classe ClevrSimplified gera um conjunto de dados sint√©ticos que simulam objetos\n",
        "# com diversas propriedades (posi√ß√£o, cor, forma, tamanho). Cada objeto √© um vetor.\n",
        "class ClevrSimplified:\n",
        "    def __init__(self, num_samples=50):\n",
        "        self.num_samples = num_samples\n",
        "        self.data = self._generate()\n",
        "\n",
        "    # M√©todo privado para gerar os vetores de caracter√≠sticas para cada objeto.\n",
        "    # Cada vetor cont√©m: [pos_x, pos_y, color_r, color_g, color_b, shape_circ, shape_sq, shape_cyl, shape_cone, shape_tri, size]\n",
        "    def _generate(self):\n",
        "        data = []\n",
        "        for _ in range(self.num_samples):\n",
        "            pos = np.random.rand(2) # Posi√ß√£o (x, y) normalizada entre 0 e 1\n",
        "            c_idx = np.random.randint(0, 3) # √çndice da cor (0:R, 1:G, 2:B)\n",
        "            color = np.zeros(3); color[c_idx] = 1 # Representa√ß√£o one-hot da cor\n",
        "            s_idx = np.random.randint(0, 5) # √çndice da forma (0:Circle, 1:Square, etc.)\n",
        "            shape = np.zeros(5); shape[s_idx] = 1 # Representa√ß√£o one-hot da forma\n",
        "            is_large = np.random.rand() > 0.5 # Booleano para definir se o tamanho √© grande\n",
        "            size = np.array([1.0 if is_large else 0.0]) # Tamanho (1.0 para grande, 0.0 para pequeno)\n",
        "            vec = np.concatenate([pos, color, shape, size]) # Concatena todas as caracter√≠sticas em um √∫nico vetor\n",
        "            data.append(vec)\n",
        "        # Converte a lista de vetores para um tensor PyTorch e move para o dispositivo configurado.\n",
        "        return torch.tensor(np.array(data), dtype=torch.float32).to(device)\n",
        "\n",
        "# --- DEFINI√á√ÉO DOS MODELOS NEURAIS PARA PREDICADOS LTN ---\n",
        "# Estes modelos s√£o usados dentro dos predicados LTN para inferir a verdade\n",
        "# de proposi√ß√µes (e.g., '√â um c√≠rculo', 'Est√° √† esquerda de').\n",
        "\n",
        "# FeatureModel: Modelo neural gen√©rico para predicados un√°rios (propriedades de um √∫nico objeto).\n",
        "# Recebe um subconjunto das caracter√≠sticas do objeto (definido por input_indices).\n",
        "class FeatureModel(nn.Module):\n",
        "    def __init__(self, input_indices):\n",
        "        super(FeatureModel, self).__init__()\n",
        "        self.indices = input_indices # √çndices das caracter√≠sticas a serem usadas (e.g., [5] para IsCircle)\n",
        "        input_dim = len(input_indices) # Dimens√£o da entrada do modelo\n",
        "        self.net = nn.Sequential( # Rede neural simples com uma camada oculta e ELU/Sigmoid.\n",
        "            nn.Linear(input_dim, 16), nn.ELU(), nn.Linear(16, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # Seleciona as caracter√≠sticas relevantes e passa pela rede neural.\n",
        "        return self.net(x[..., self.indices])\n",
        "\n",
        "# RelationModel: Modelo neural gen√©rico para predicados bin√°rios (rela√ß√µes entre dois objetos).\n",
        "# Recebe caracter√≠sticas de dois objetos (e.g., posi√ß√µes para LeftOf).\n",
        "class RelationModel(nn.Module):\n",
        "    def __init__(self, axis_idx):\n",
        "        super(RelationModel, self).__init__()\n",
        "        self.axis = axis_idx # Eixo a ser considerado (0 para X, 1 para Y)\n",
        "        self.net = nn.Sequential( # Rede neural para combinar as caracter√≠sticas dos dois objetos.\n",
        "            nn.Linear(2, 16), nn.ELU(), nn.Linear(16, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x, y):\n",
        "        # Concatena a caracter√≠stica do eixo de ambos os objetos e passa pela rede.\n",
        "        return self.net(torch.cat([x[..., self.axis:self.axis+1], y[..., self.axis:self.axis+1]], dim=-1))\n",
        "\n",
        "# CloseToModel: Predicado espec√≠fico para verificar a proximidade entre dois objetos.\n",
        "# Calcula a dist√¢ncia euclidiana entre as posi√ß√µes (primeiros 2 √≠ndices) dos objetos.\n",
        "class CloseToModel(nn.Module):\n",
        "    def forward(self, x, y):\n",
        "        # Calcula o quadrado da dist√¢ncia entre os vetores de posi√ß√£o (0:2).\n",
        "        dist_sq = torch.sum((x[..., 0:2] - y[..., 0:2])**2, dim=-1, keepdim=True)\n",
        "        # Retorna um valor baseado na exponencial negativa da dist√¢ncia (quanto menor a dist√¢ncia, mais pr√≥ximo de 1).\n",
        "        return torch.exp(-2.0 * dist_sq)\n",
        "\n",
        "# SameSizeModel: Predicado espec√≠fico para verificar se dois objetos t√™m o mesmo tamanho.\n",
        "# Compara o valor da caracter√≠stica de tamanho (√≠ndice 10) dos objetos.\n",
        "class SameSizeModel(nn.Module):\n",
        "    def forward(self, x, y):\n",
        "        # Retorna 1 menos o valor absoluto da diferen√ßa de tamanho (1 se iguais, 0 se muito diferentes).\n",
        "        return 1.0 - torch.abs(x[..., 10:11] - y[..., 10:11])\n",
        "\n",
        "print(\"Modelos neurais e classe de gera√ß√£o de dados definidos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_5",
        "outputId": "a465d406-2f51-4988-c120-7ecb6d627b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ocorreu um erro ao carregar ou pr√©-processar a imagem: name 'image_path' is not defined\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Caminho da imagem (atualizado para a imagem do usu√°rio)\n",
        "# `image_path` √© atualizado na c√©lula 00b9437a\n",
        "\n",
        "# Define as transforma√ß√µes de pr√©-processamento para a imagem:\n",
        "# 1. Redimensiona a imagem para (224, 224) pixels - tamanho comum para modelos de vis√£o.\n",
        "# 2. Converte a imagem PIL para um tensor PyTorch.\n",
        "# 3. Normaliza o tensor com m√©dia e desvio padr√£o padr√£o para modelos pr√©-treinados em ImageNet.\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "try:\n",
        "    # Carregar a imagem do caminho especificado e converter para o formato RGB.\n",
        "    original_image = Image.open(image_path).convert(\"RGB\") # Usar 'original_image' para consist√™ncia.\n",
        "    print(f\"Imagem carregada do caminho: {image_path}\")\n",
        "\n",
        "    # Aplica as transforma√ß√µes definidas √† imagem.\n",
        "    preprocessed_image = preprocess(original_image)\n",
        "\n",
        "    # Adiciona uma dimens√£o de batch ao tensor pr√©-processado (modelos de rede neural geralmente esperam um batch de imagens).\n",
        "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
        "\n",
        "    print(f\"Imagem pr√©-processada com sucesso. Formato final: {preprocessed_image.shape}\")\n",
        "    print(f\"Tipo de dado: {preprocessed_image.dtype}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: Imagem n√£o encontrada no caminho: {image_path}\")\n",
        "    original_image = None # Define como None em caso de erro\n",
        "    preprocessed_image = None\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro ao carregar ou pr√©-processar a imagem: {e}\")\n",
        "    original_image = None # Define como None em caso de erro\n",
        "    preprocessed_image = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_7",
        "outputId": "04637ced-ead8-490e-be20-ef16b9f6156e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.238)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Carregando modelo YOLOv5 no dispositivo: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/hub.py:335: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 üöÄ 2025-12-15 Python-3.12.12 torch-2.9.0+cpu CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1M/14.1M [00:00<00:00, 99.5MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo YOLOv5 carregado com sucesso.\n",
            "Erro: A imagem original n√£o foi carregada. Por favor, execute new_cell_5 primeiro.\n",
            "Nenhum objeto detectado, usando objeto dummy como fallback.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Garante que ultralytics esteja instalado para YOLOv5\n",
        "# A instala√ß√£o √© importante para que o torch.hub.load funcione corretamente.\n",
        "!pip install ultralytics\n",
        "\n",
        "# Garante que o dispositivo est√° dispon√≠vel (definido no notebook principal)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# `device` j√° foi definido na c√©lula 1 e est√° dispon√≠vel globalmente.\n",
        "\n",
        "print(f\"Carregando modelo YOLOv5 no dispositivo: {device}\")\n",
        "# 1. Carregar um modelo pr√©-treinado de detec√ß√£o de objetos (YOLOv5).\n",
        "# Usamos `torch.hub.load` que gerencia o download e carregamento do modelo 'yolov5s' (vers√£o small).\n",
        "# `pretrained=True` garante que o modelo venha com pesos pr√©-treinados no dataset COCO.\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)\n",
        "model.eval() # Definir o modelo para modo de avalia√ß√£o para desativar o dropout e normaliza√ß√£o em batch.\n",
        "\n",
        "print(\"Modelo YOLOv5 carregado com sucesso.\")\n",
        "\n",
        "# A imagem original `original_image` √© carregada na c√©lula `new_cell_5`.\n",
        "# Garantir que `original_image` esteja dispon√≠vel\n",
        "if original_image is None:\n",
        "    print(\"Erro: A imagem original n√£o foi carregada. Por favor, execute new_cell_5 primeiro.\")\n",
        "    # Pode ser necess√°rio adicionar uma l√≥gica de fallback ou parar aqui\n",
        "    # Para evitar que o restante da c√©lula falhe, vamos usar um objeto dummy se original_image n√£o estiver dispon√≠vel.\n",
        "    # No entanto, a instru√ß√£o para reexecutar new_cell_5 deve resolver isso.\n",
        "    detected_objects_features = torch.tensor([[0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5]], dtype=torch.float32).to(device)\n",
        "    print(\"Nenhum objeto detectado, usando objeto dummy como fallback.\")\n",
        "else:\n",
        "    print(\"Realizando infer√™ncia do modelo...\")\n",
        "    # 2. Passar a imagem original pelo modelo de detec√ß√£o de objetos.\n",
        "    # O `size=224` redimensiona a imagem internamente para a infer√™ncia.\n",
        "    results = model(original_image, size=224)\n",
        "\n",
        "    # Os resultados s√£o uma lista de objetos 'Detection'. `results.pred` cont√©m as detec√ß√µes\n",
        "    # formatadas como [xmin, ymin, xmax, ymax, confidence, class_id].\n",
        "    detections = results.pred[0] # Pega as detec√ß√µes para a primeira (e √∫nica) imagem do batch.\n",
        "\n",
        "    print(f\"N√∫mero total de objetos detectados (antes do filtro): {len(detections)}\")\n",
        "\n",
        "    # Definir um threshold de confian√ßa para filtrar detec√ß√µes fracas.\n",
        "    # `confidence_threshold` √© uma vari√°vel do kernel com valor 0.5 (definido em uma c√©lula anterior).\n",
        "    filtered_detections = detections[detections[:, 4] > confidence_threshold]\n",
        "\n",
        "    print(f\"N√∫mero de objetos detectados ap√≥s filtro de confian√ßa ({confidence_threshold}): {len(filtered_detections)}\")\n",
        "\n",
        "    # --- EXTRA√á√ÉO E NORMALIZA√á√ÉO DE CARACTER√çSTICAS PARA O FORMATO LTN ---\n",
        "    # O formato LTN esperado √©: [pos_x, pos_y, color_r, color_g, color_b, shape_circ, shape_sq, shape_cyl, shape_cone, shape_tri, size]\n",
        "\n",
        "    # Se n√£o houver detec√ß√µes com confian√ßa suficiente, cria um objeto dummy para evitar erros.\n",
        "    if len(filtered_detections) == 0:\n",
        "        print(\"Nenhum objeto detectado com confian√ßa suficiente. Criando um objeto dummy.\")\n",
        "        # Objeto dummy: centro, verde, quadrado, tamanho m√©dio.\n",
        "        # pos_x=0.5, pos_y=0.5, color_r=0, color_g=1, color_b=0, shape_circ=0, shape_sq=1, shape_cyl=0, shape_cone=0, shape_tri=0, size=0.5\n",
        "        detected_objects_features = torch.tensor([[0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5]], dtype=torch.float32).to(device) # Corrigi o shape_sq para 0.0 para ser mais gen√©rico ao dummy, ou mantive como era no original: 1.0 para quadrado\n",
        "    else:\n",
        "        detected_objects_features = []\n",
        "        img_width, img_height = original_image.size\n",
        "\n",
        "        # Mapeamento de classes YOLOv5 (COCO dataset) para 'formas' ClevrSimplified.\n",
        "        # Esta √© uma simplifica√ß√£o, pois YOLOv5 n√£o detecta formas geom√©tricas abstratas diretamente.\n",
        "        # As classes de ClevrSimplified s√£o: IsCircle(0), IsSquare(1), IsCylinder(2), IsCone(3), IsTriangle(4).\n",
        "        # Exemplos de mapeamento para algumas classes comuns do COCO:\n",
        "        yolo_class_to_clevr_shape = {\n",
        "            2: 1, # 'car' -> IsSquare\n",
        "            3: 1, # 'motorcycle' -> IsSquare (simplifica√ß√£o)\n",
        "            7: 1, # 'truck' -> IsSquare\n",
        "            5: 0, # 'bus' -> IsCircle (simplifica√ß√£o, para ter variedade)\n",
        "            0: 2, # 'person' -> IsCylinder (simplifica√ß√£o)\n",
        "            14: 3, # 'bird' -> IsCone (simplifica√ß√£o)\n",
        "            # Adicione mais mapeamentos conforme a necessidade e a interpreta√ß√£o.\n",
        "        }\n",
        "\n",
        "        for *xyxy, conf, cls_id in filtered_detections:\n",
        "            bbox = [val.item() for val in xyxy] # Extrai coordenadas da caixa delimitadora.\n",
        "            xmin, ymin, xmax, ymax = bbox\n",
        "\n",
        "            # a. Posi√ß√£o (centro da caixa delimitadora, normalizada para 0-1).\n",
        "            center_x = (xmin + xmax) / 2 / img_width\n",
        "            center_y = (ymin + ymax) / 2 / img_height\n",
        "            pos_features = [center_x, center_y]\n",
        "\n",
        "            # b. Cor (m√©dia RGB da caixa delimitadora, simplificado para 3 componentes one-hot-like).\n",
        "            cropped_img_pil = original_image.crop((xmin, ymin, xmax, ymax)) # Recorta a √°rea do objeto.\n",
        "            cropped_img_np = np.array(cropped_img_pil) # Converte para array NumPy.\n",
        "\n",
        "            if cropped_img_np.size == 0: # Caso a √°rea recortada seja vazia.\n",
        "                avg_rgb = [0.0, 0.0, 0.0]\n",
        "            else:\n",
        "                avg_rgb = np.mean(cropped_img_np, axis=(0, 1)) / 255.0 # Calcula a m√©dia RGB e normaliza para 0-1.\n",
        "\n",
        "            # Simplifica√ß√£o para um formato one-hot-like de 3 cores (R, G, B).\n",
        "            color_features = [0.0, 0.0, 0.0]\n",
        "            if avg_rgb[0] > avg_rgb[1] and avg_rgb[0] > avg_rgb[2]:\n",
        "                color_features[0] = 1.0 # Vermelho dominante.\n",
        "            elif avg_rgb[1] > avg_rgb[0] and avg_rgb[1] > avg_rgb[2]:\n",
        "                color_features[1] = 1.0 # Verde dominante.\n",
        "            elif avg_rgb[2] > avg_rgb[0] and avg_rgb[2] > avg_rgb[1]:\n",
        "                color_features[2] = 1.0 # Azul dominante.\n",
        "            else:\n",
        "                color_features[0] = 1.0 # Padr√£o para Vermelho se n√£o houver domin√¢ncia clara.\n",
        "\n",
        "            # c. Forma (do r√≥tulo de classe do YOLOv5, mapeado para Clevr).\n",
        "            shape_one_hot = [0.0] * 5 # [IsCircle, IsSquare, IsCylinder, IsCone, IsTriangle]\n",
        "            clevr_shape_idx = yolo_class_to_clevr_shape.get(int(cls_id.item()), -1) # Obt√©m o √≠ndice da forma Clevr.\n",
        "            if 0 <= clevr_shape_idx < 5:\n",
        "                shape_one_hot[clevr_shape_idx] = 1.0\n",
        "            else:\n",
        "                shape_one_hot[1] = 1.0 # Padr√£o para IsSquare se a classe YOLO n√£o for mapeada.\n",
        "\n",
        "            # d. Tamanho (√°rea da caixa delimitadora, normalizada e mapeada para 'is_large' ou 'is_small').\n",
        "            area = (xmax - xmin) * (ymax - ymin)\n",
        "            normalized_area = area / (img_width * img_height) # Normaliza a √°rea pela √°rea total da imagem.\n",
        "            is_large_feature = 1.0 if normalized_area > 0.05 else 0.0 # Threshold arbitr√°rio para definir 'grande' ou 'pequeno'.\n",
        "            size_features = [is_large_feature]\n",
        "\n",
        "            # 4. Concatena todas as caracter√≠sticas para formar o vetor do objeto.\n",
        "            object_vec = pos_features + color_features + shape_one_hot + size_features\n",
        "            detected_objects_features.append(object_vec)\n",
        "\n",
        "        # Converte a lista de vetores de caracter√≠sticas para um tensor PyTorch.\n",
        "        detected_objects_features = torch.tensor(detected_objects_features, dtype=torch.float32).to(device)\n",
        "\n",
        "    print(f\"Caracter√≠sticas extra√≠das para {detected_objects_features.shape[0]} objetos.\")\n",
        "    print(f\"Formato do tensor de caracter√≠sticas dos objetos: {detected_objects_features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "165e9c9d"
      },
      "source": [
        "## 4. Visualizar Objetos Detectados e Suas Caracter√≠sticas\n",
        "\n",
        "### Subtask:\n",
        "Visualizar a imagem com os objetos detectados (caixas delimitadoras) e exibir suas caracter√≠sticas extra√≠das (posi√ß√£o, cor, forma, tamanho)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8df8238e",
        "outputId": "2bf967ae-a5a2-4599-8c69-02b5bb8901b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erro: original_image n√£o encontrada. Por favor, execute as c√©lulas de carregamento de imagem primeiro.\n",
            "--- Detalhes dos Objetos ---\n",
            "Nenhum objeto real detectado com confian√ßa suficiente. Visualizando o objeto dummy.\n",
            "Objeto Dummy:\n",
            "  - Posi√ß√£o (norm): (0.50, 0.50)\n",
            "  - Cor inferida: Verde\n",
            "  - Forma inferida: Quadrado\n",
            "  - Tamanho inferido: Pequeno\n",
            "  - Nota: Sem confian√ßa de detec√ß√£o real, este √© um placeholder.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Usa a imagem original j√° carregada na c√©lula `new_cell_5` ou `new_cell_7`.\n",
        "# `original_image` deve estar dispon√≠vel globalmente.\n",
        "if 'original_image' not in locals() or original_image is None:\n",
        "    print(\"Erro: original_image n√£o encontrada. Por favor, execute as c√©lulas de carregamento de imagem primeiro.\")\n",
        "    # Fallback para evitar erro, mas o usu√°rio deve reexecutar a c√©lula new_cell_5\n",
        "    original_image = Image.new('RGB', (600, 400), color = 'white') # Cria uma imagem em branco\n",
        "    img_width, img_height = original_image.size\n",
        "else:\n",
        "    img_width, img_height = original_image.size\n",
        "\n",
        "# Cria uma figura e um eixo para exibir a imagem\n",
        "fig, ax = plt.subplots(1, figsize=(12, 12))\n",
        "ax.imshow(original_image)\n",
        "ax.set_title(\"Objetos Detectados e Caracter√≠sticas\")\n",
        "ax.axis('off') # Remove os eixos\n",
        "\n",
        "print(\"--- Detalhes dos Objetos ---\")\n",
        "\n",
        "# Itera sobre os objetos em detected_objects_features para visualiza√ß√£o\n",
        "# `detected_objects_features` √© o tensor que cont√©m [pos_x, pos_y, color_r, color_g, color_b, shape_circ, shape_sq, shape_cyl, shape_cone, shape_tri, size]\n",
        "# Lembre-se que `filtered_detections` (do YOLO) √© a fonte original das bboxes para objetos reais.\n",
        "\n",
        "if 'filtered_detections' in locals() and len(filtered_detections) > 0: # Caso objetos reais tenham sido detectados\n",
        "    for i, (*xyxy, conf, cls_id) in enumerate(filtered_detections):\n",
        "        bbox = [val.item() for val in xyxy]  # xmin, ymin, xmax, ymax\n",
        "        xmin, ymin, xmax, ymax = bbox\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        # Desenha a caixa delimitadora\n",
        "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='lime', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Obt√©m as caracter√≠sticas do objeto no formato LTN para exibi√ß√£o\n",
        "        obj_feature_vec = detected_objects_features[i]\n",
        "\n",
        "        # Posi√ß√£o\n",
        "        pos_x_norm = obj_feature_vec[0].item()\n",
        "        pos_y_norm = obj_feature_vec[1].item()\n",
        "        center_x_pixel = pos_x_norm * img_width\n",
        "        center_y_pixel = pos_y_norm * img_height\n",
        "\n",
        "        # Cor (√≠ndices 2, 3, 4)\n",
        "        colors_map = {0:'Vermelho', 1:'Verde', 2:'Azul'}\n",
        "        color_idx = torch.argmax(obj_feature_vec[2:5]).item()\n",
        "        inferred_color = colors_map.get(color_idx, 'Desconhecida')\n",
        "\n",
        "        # Forma (√≠ndices 5, 6, 7, 8, 9)\n",
        "        shapes_map = {0:'C√≠rculo', 1:'Quadrado', 2:'Cilindro', 3:'Cone', 4:'Tri√¢ngulo'}\n",
        "        shape_idx = torch.argmax(obj_feature_vec[5:10]).item()\n",
        "        inferred_shape = shapes_map.get(shape_idx, 'Desconhecida')\n",
        "\n",
        "        # Tamanho (√≠ndice 10)\n",
        "        inferred_size = \"Grande\" if obj_feature_vec[10].item() == 1.0 else \"Pequeno\"\n",
        "\n",
        "        # Texto para o r√≥tulo\n",
        "        label_text = f\"Objeto {i+1}:\\nPos: ({pos_x_norm:.2f}, {pos_y_norm:.2f})\\nCor: {inferred_color}\\nForma: {inferred_shape}\\nTamanho: {inferred_size}\"\n",
        "        ax.text(xmin, ymin - 10, label_text, color='lime', fontsize=9, bbox=dict(facecolor='black', alpha=0.7, edgecolor='none'))\n",
        "\n",
        "        print(f\"Objeto {i+1} (Original YOLO class: {model.names[int(cls_id.item())]}):\")\n",
        "        print(f\"  - Posi√ß√£o (norm): ({pos_x_norm:.2f}, {pos_y_norm:.2f})\")\n",
        "        print(f\"  - Cor inferida: {inferred_color}\")\n",
        "        print(f\"  - Forma inferida: {inferred_shape}\")\n",
        "        print(f\"  - Tamanho inferido: {inferred_size}\")\n",
        "        print(f\"  - Confian√ßa da detec√ß√£o: {conf.item():.2f}\")\n",
        "\n",
        "else: # Caso apenas o objeto dummy esteja presente\n",
        "    print(\"Nenhum objeto real detectado com confian√ßa suficiente. Visualizando o objeto dummy.\")\n",
        "    obj_feature_vec = detected_objects_features[0] # Pega as caracter√≠sticas do dummy\n",
        "\n",
        "    # Posi√ß√£o do dummy (centro da imagem)\n",
        "    pos_x_norm = obj_feature_vec[0].item()\n",
        "    pos_y_norm = obj_feature_vec[1].item()\n",
        "    center_x_pixel = pos_x_norm * img_width\n",
        "    center_y_pixel = pos_y_norm * img_height\n",
        "\n",
        "    # Cor do dummy\n",
        "    colors_map = {0:'Vermelho', 1:'Verde', 2:'Azul'}\n",
        "    color_idx = torch.argmax(obj_feature_vec[2:5]).item()\n",
        "    inferred_color = colors_map.get(color_idx, 'Desconhecida')\n",
        "\n",
        "    # Forma do dummy\n",
        "    shapes_map = {0:'C√≠rculo', 1:'Quadrado', 2:'Cilindro', 3:'Cone', 4:'Tri√¢ngulo'}\n",
        "    shape_idx = torch.argmax(obj_feature_vec[5:10]).item()\n",
        "    inferred_shape = shapes_map.get(shape_idx, 'Desconhecida')\n",
        "\n",
        "    # Tamanho do dummy\n",
        "    inferred_size = \"Grande\" if obj_feature_vec[10].item() == 1.0 else \"Pequeno\"\n",
        "\n",
        "    # Desenha um ponto para representar o objeto dummy (sem bbox real)\n",
        "    ax.plot(center_x_pixel, center_y_pixel, 'ro', markersize=15, label='Objeto Dummy')\n",
        "    label_text = f\"Objeto Dummy:\\nPos: ({pos_x_norm:.2f}, {pos_y_norm:.2f})\\nCor: {inferred_color}\\nForma: {inferred_shape}\\nTamanho: {inferred_size}\"\n",
        "    ax.text(center_x_pixel + 20, center_y_pixel - 20, label_text, color='red', fontsize=10, bbox=dict(facecolor='black', alpha=0.7, edgecolor='none'))\n",
        "    ax.legend()\n",
        "\n",
        "    print(f\"Objeto Dummy:\")\n",
        "    print(f\"  - Posi√ß√£o (norm): ({pos_x_norm:.2f}, {pos_y_norm:.2f})\")\n",
        "    print(f\"  - Cor inferida: {inferred_color}\")\n",
        "    print(f\"  - Forma inferida: {inferred_shape}\")\n",
        "    print(f\"  - Tamanho inferido: {inferred_size}\")\n",
        "    print(\"  - Nota: Sem confian√ßa de detec√ß√£o real, este √© um placeholder.\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27a2cdb2"
      },
      "source": [
        "## 5. Tentar Nova Imagem com Objetos Detect√°veis\n",
        "\n",
        "### Subtask:\n",
        "Baixar uma nova imagem com objetos claros e vis√≠veis para teste de detec√ß√£o e atualizar a vari√°vel `image_path`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_11",
        "outputId": "d36bbc89-4e66-415a-d709-75bf7c3f12d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Detalhes dos Objetos ---\n",
            "Nenhum objeto real detectado com confian√ßa suficiente. Visualizando o objeto dummy.\n",
            "Objeto Dummy:\n",
            "  - Posi√ß√£o (norm): (0.50, 0.50)\n",
            "  - Cor inferida: Verde\n",
            "  - Forma inferida: Quadrado\n",
            "  - Tamanho inferido: {inferred_size}\n",
            "  - Nota: Sem confian√ßa de detec√ß√£o real, este √© um placeholder.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Usa a imagem original j√° carregada na c√©lula `new_cell_5` ou `new_cell_7`.\n",
        "# `original_image` deve estar dispon√≠vel globalmente.\n",
        "if 'original_image' not in locals() or original_image is None:\n",
        "    print(\"Erro: original_image n√£o encontrada. Por favor, execute as c√©lulas de carregamento de imagem primeiro.\")\n",
        "    # Fallback para evitar erro, mas o usu√°rio deve reexecutar a c√©lula new_cell_5\n",
        "    original_image = Image.new('RGB', (600, 400), color = 'white') # Cria uma imagem em branco\n",
        "    img_width, img_height = original_image.size\n",
        "else:\n",
        "    img_width, img_height = original_image.size\n",
        "\n",
        "# Cria uma figura e um eixo para exibir a imagem\n",
        "fig, ax = plt.subplots(1, figsize=(12, 12))\n",
        "ax.imshow(original_image)\n",
        "ax.set_title(\"Objetos Detectados e Caracter√≠sticas\")\n",
        "ax.axis('off') # Remove os eixos\n",
        "\n",
        "print(\"--- Detalhes dos Objetos ---\")\n",
        "\n",
        "# Itera sobre os objetos em detected_objects_features para visualiza√ß√£o\n",
        "# `detected_objects_features` √© o tensor que cont√©m [pos_x, pos_y, color_r, color_g, color_b, shape_circ, shape_sq, shape_cyl, shape_cone, shape_tri, size]\n",
        "# Lembre-se que `filtered_detections` (do YOLO) √© a fonte original das bboxes para objetos reais.\n",
        "\n",
        "if 'filtered_detections' in locals() and len(filtered_detections) > 0: # Caso objetos reais tenham sido detectados\n",
        "    for i, (*xyxy, conf, cls_id) in enumerate(filtered_detections):\n",
        "        bbox = [val.item() for val in xyxy]  # xmin, ymin, xmax, ymax\n",
        "        xmin, ymin, xmax, ymax = bbox\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        # Desenha a caixa delimitadora\n",
        "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='lime', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Obt√©m as caracter√≠sticas do objeto no formato LTN para exibi√ß√£o\n",
        "        obj_feature_vec = detected_objects_features[i]\n",
        "\n",
        "        # Posi√ß√£o\n",
        "        pos_x_norm = obj_feature_vec[0].item()\n",
        "        pos_y_norm = obj_feature_vec[1].item()\n",
        "        center_x_pixel = pos_x_norm * img_width\n",
        "        center_y_pixel = pos_y_norm * img_height\n",
        "\n",
        "        # Cor (√≠ndices 2, 3, 4)\n",
        "        colors_map = {0:'Vermelho', 1:'Verde', 2:'Azul'}\n",
        "        color_idx = torch.argmax(obj_feature_vec[2:5]).item()\n",
        "        inferred_color = colors_map.get(color_idx, 'Desconhecida')\n",
        "\n",
        "        # Forma (√≠ndices 5, 6, 7, 8, 9)\n",
        "        shapes_map = {0:'C√≠rculo', 1:'Quadrado', 2:'Cilindro', 3:'Cone', 4:'Tri√¢ngulo'}\n",
        "        shape_idx = torch.argmax(obj_feature_vec[5:10]).item()\n",
        "        inferred_shape = shapes_map.get(shape_idx, 'Desconhecida')\n",
        "\n",
        "        # Tamanho (√≠ndice 10)\n",
        "        inferred_size = \"Grande\" if obj_feature_vec[10].item() == 1.0 else \"Pequeno\"\n",
        "\n",
        "        # Texto para o r√≥tulo\n",
        "        label_text = f\"Objeto {i+1}:\\nPos: ({pos_x_norm:.2f}, {pos_y_norm:.2f})\\nCor: {inferred_color}\\nForma: {inferred_shape}\\nTamanho: {inferred_size}\"\n",
        "        ax.text(xmin, ymin - 10, label_text, color='lime', fontsize=9, bbox=dict(facecolor='black', alpha=0.7, edgecolor='none'))\n",
        "\n",
        "        print(f\"Objeto {i+1} (Original YOLO class: {model.names[int(cls_id.item())]}):\")\n",
        "        print(f\"  - Posi√ß√£o (norm): ({pos_x_norm:.2f}, {pos_y_norm:.2f})\")\n",
        "        print(f\"  - Cor inferida: {inferred_color}\")\n",
        "        print(f\"  - Forma inferida: {inferred_shape}\")\n",
        "        print(f\"  - Tamanho inferido: {inferred_size}\")\n",
        "        print(f\"  - Confian√ßa da detec√ß√£o: {conf.item():.2f}\")\n",
        "\n",
        "else: # Caso apenas o objeto dummy esteja presente\n",
        "    print(\"Nenhum objeto real detectado com confian√ßa suficiente. Visualizando o objeto dummy.\")\n",
        "    obj_feature_vec = detected_objects_features[0] # Pega as caracter√≠sticas do dummy\n",
        "\n",
        "    # Posi√ß√£o do dummy (centro da imagem)\n",
        "    pos_x_norm = obj_feature_vec[0].item()\n",
        "    pos_y_norm = obj_feature_vec[1].item()\n",
        "    center_x_pixel = pos_x_norm * img_width\n",
        "    center_y_pixel = pos_y_norm * img_height\n",
        "\n",
        "    # Cor do dummy\n",
        "    colors_map = {0:'Vermelho', 1:'Verde', 2:'Azul'}\n",
        "    color_idx = torch.argmax(obj_feature_vec[2:5]).item()\n",
        "    inferred_color = colors_map.get(color_idx, 'Desconhecida')\n",
        "\n",
        "    # Forma do dummy\n",
        "    shapes_map = {0:'C√≠rculo', 1:'Quadrado', 2:'Cilindro', 3:'Cone', 4:'Tri√¢ngulo'}\n",
        "    shape_idx = torch.argmax(obj_feature_vec[5:10]).item()\n",
        "    inferred_shape = shapes_map.get(shape_idx, 'Desconhecida')\n",
        "\n",
        "    # Tamanho do dummy\n",
        "    inferred_size = \"Grande\" if obj_feature_vec[10].item() == 1.0 else \"Pequeno\"\n",
        "\n",
        "    # Desenha um ponto para representar o objeto dummy (sem bbox real)\n",
        "    ax.plot(center_x_pixel, center_y_pixel, 'ro', markersize=15, label='Objeto Dummy')\n",
        "    label_text = f\"Objeto Dummy:\\nPos: ({pos_x_norm:.2f}, {pos_y_norm:.2f})\\nCor: {inferred_color}\\nForma: {inferred_shape}\\nTamanho: {inferred_size}\"\n",
        "    ax.text(center_x_pixel + 20, center_y_pixel - 20, label_text, color='red', fontsize=10, bbox=dict(facecolor='black', alpha=0.7, edgecolor='none'))\n",
        "    ax.legend()\n",
        "\n",
        "    print(f\"Objeto Dummy:\")\n",
        "    print(f\"  - Posi√ß√£o (norm): ({pos_x_norm:.2f}, {pos_y_norm:.2f})\")\n",
        "    print(f\"  - Cor inferida: {inferred_color}\")\n",
        "    print(f\"  - Forma inferida: {inferred_shape}\")\n",
        "    print(\"  - Tamanho inferido: {inferred_size}\")\n",
        "    print(\"  - Nota: Sem confian√ßa de detec√ß√£o real, este √© um placeholder.\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c104e10",
        "outputId": "aabb6960-0b89-482b-e448-b45391e2b0c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confian√ßa m√≠nima para detec√ß√£o de objetos definida como: 0.5\n"
          ]
        }
      ],
      "source": [
        "# --- Define a confian√ßa m√≠nima para detec√ß√£o de objetos ---\n",
        "# Este valor √© usado na c√©lula `new_cell_7` para filtrar detec√ß√µes do YOLOv5.\n",
        "confidence_threshold = 0.5\n",
        "print(f\"Confian√ßa m√≠nima para detec√ß√£o de objetos definida como: {confidence_threshold}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_9",
        "outputId": "77f04ac5-ebc7-4ca7-fc86-9cae57c0faf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'detected_objects_features' encontrado com 1 objetos.\n",
            "ImageObjectDataset inicializado com 1 objetos.\n",
            "Formato dos dados: torch.Size([1, 11])\n",
            "Dataset para imagem real criado com sucesso. Cont√©m 1 objetos de formato torch.Size([1, 11]).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# --- CLASSE ImageObjectDataset para integra√ß√£o de dados da imagem ---\n",
        "# Esta classe √© uma adapta√ß√£o da ClevrSimplified, mas em vez de gerar dados sint√©ticos,\n",
        "# ela aceita um tensor de caracter√≠sticas de objetos j√° detectados na imagem.\n",
        "class ImageObjectDataset:\n",
        "    def __init__(self, detected_features_tensor):\n",
        "        # Armazena o tensor de caracter√≠sticas e o move para o dispositivo configurado (CPU/GPU).\n",
        "        self._data = detected_features_tensor.to(device);\n",
        "        # Define o n√∫mero de amostras com base na primeira dimens√£o do tensor.\n",
        "        self.num_samples = self.data.shape[0];\n",
        "        print(f\"ImageObjectDataset inicializado com {self.num_samples} objetos.\");\n",
        "        print(f\"Formato dos dados: {self.data.shape}\");\n",
        "\n",
        "    # Propriedade 'data' para compatibilidade com o framework LTN, permitindo acesso aos dados.\n",
        "    @property\n",
        "    def data(self):\n",
        "        return self._data;\n",
        "\n",
        "    # Setter para a propriedade 'data', embora n√£o seja usado neste contexto, √© bom manter.\n",
        "    @data.setter\n",
        "    def data(self, value):\n",
        "        self._data = value;\n",
        "\n",
        "\n",
        "# Verifica se a vari√°vel `detected_objects_features` est√° dispon√≠vel e cont√©m dados.\n",
        "# Esta vari√°vel deve ter sido gerada na etapa anterior de extra√ß√£o de caracter√≠sticas.\n",
        "if 'detected_objects_features' in locals() and detected_objects_features is not None and detected_objects_features.shape[0] > 0:\n",
        "    print(f\"'detected_objects_features' encontrado com {detected_objects_features.shape[0]} objetos.\");\n",
        "    # Instancia ImageObjectDataset com as caracter√≠sticas extra√≠das da imagem.\n",
        "    ds_image = ImageObjectDataset(detected_objects_features);\n",
        "    print(f\"Dataset para imagem real criado com sucesso. Cont√©m {ds_image.num_samples} objetos de formato {ds_image.data.shape}.\");\n",
        "else:\n",
        "    print(\"'detected_objects_features' n√£o foi encontrado ou est√° vazio. Criando um objeto dummy como fallback.\");\n",
        "    # Fallback: Se n√£o houver objetos detectados, cria um objeto dummy para que o LTN possa operar sem erros.\n",
        "    # Formato do dummy: [pos_x, pos_y, color_r, color_g, color_b, shape_circ, shape_sq, shape_cyl, shape_cone, shape_tri, size]\n",
        "    # Exemplo: um objeto no centro, verde, quadrado, tamanho m√©dio.\n",
        "    dummy_features = torch.tensor([[0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5]], dtype=torch.float32).to(device);\n",
        "    ds_image = ImageObjectDataset(dummy_features);\n",
        "    print(f\"Criado dataset dummy como fallback. Cont√©m {ds_image.num_samples} objeto de formato {ds_image.data.shape}.\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00b9437a",
        "outputId": "6ab6a82a-61f8-4025-b5bc-6375eb94667c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-15 13:37:15--  https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/zidane.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168949 (165K) [image/jpeg]\n",
            "Saving to: ‚Äò/content/multiple_objects.jpg‚Äô\n",
            "\n",
            "/content/multiple_o 100%[===================>] 164.99K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-12-15 13:37:15 (11.0 MB/s) - ‚Äò/content/multiple_objects.jpg‚Äô saved [168949/168949]\n",
            "\n",
            "Caminho da imagem atualizado para: /content/multiple_objects.jpg\n",
            "Por favor, execute as c√©lulas `new_cell_5`, `new_cell_7`, `new_cell_9`, `new_cell_3` e `new_cell_11` novamente para reexecutar o pipeline com a nova imagem.\n"
          ]
        }
      ],
      "source": [
        "# Baixa uma nova imagem para teste\n",
        "!wget -O /content/multiple_objects.jpg https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/zidane.jpg\n",
        "\n",
        "# Atualiza o caminho da imagem para a nova imagem baixada\n",
        "image_path = \"/content/multiple_objects.jpg\"\n",
        "\n",
        "print(f\"Caminho da imagem atualizado para: {image_path}\")\n",
        "print(\"Por favor, execute as c√©lulas `new_cell_5`, `new_cell_7`, `new_cell_9`, `new_cell_3` e `new_cell_11` novamente para reexecutar o pipeline com a nova imagem.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_3",
        "outputId": "30d10013-8512-47c1-b079-1545e62b543a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- INICIANDO EXECU√á√ÉO 1/5 ---\n",
            "Treinando...\n",
            "Loss Final Run 1: 0.0004\n",
            "\n",
            "--- INICIANDO EXECU√á√ÉO 2/5 ---\n",
            "Treinando...\n",
            "Loss Final Run 2: 0.0002\n",
            "\n",
            "--- INICIANDO EXECU√á√ÉO 3/5 ---\n",
            "Treinando...\n",
            "Loss Final Run 3: 0.0002\n",
            "\n",
            "--- INICIANDO EXECU√á√ÉO 4/5 ---\n",
            "Treinando...\n",
            "Loss Final Run 4: 0.0002\n",
            "\n",
            "--- INICIANDO EXECU√á√ÉO 5/5 ---\n",
            "Treinando...\n",
            "Loss Final Run 5: 0.0002\n",
            "\n",
            "=== RESULTADOS FINAIS (M√âDIA DE 5 EXECU√á√ïES) ===\n",
            "sat_q1: 0.0002 (+/- 0.0000)\n",
            "sat_q2: 0.0001 (+/- 0.0000)\n",
            "sat_regra_prox: 0.9999 (+/- 0.0000)\n",
            "accuracy: 1.0000 (+/- 0.0000)\n",
            "precision: 1.0000 (+/- 0.0000)\n",
            "recall: 1.0000 (+/- 0.0000)\n",
            "f1: 1.0000 (+/- 0.0000)\n"
          ]
        }
      ],
      "source": [
        "# --- 3. CONFIGURA√á√ÉO E EXECU√á√ÉO DO EXPERIMENTO LTN ---\n",
        "# Este bloco instancia os predicados LTN com os modelos neurais, define os operadores l√≥gicos,\n",
        "# configura o otimizador, executa o loop de treinamento e realiza a avalia√ß√£o final.\n",
        "\n",
        "# --- A. INSTANCIA√á√ÉO DOS PREDICADOS LTN ---\n",
        "# Cada predicado LTN encapsula um modelo neural (FeatureModel ou RelationModel)\n",
        "# que aprende a verdade de uma proposi√ß√£o. `.to(device)` move o modelo para a GPU/CPU.\n",
        "\n",
        "# Predicados Un√°rios (Propriedades de um √∫nico objeto):\n",
        "# IsCircle, IsSquare, IsCylinder, IsCone, IsTriangle: Usam FeatureModel para checar a forma (√≠ndices 5-9).\n",
        "IsCircle = ltn.Predicate(FeatureModel([5]).to(device))\n",
        "IsSquare = ltn.Predicate(FeatureModel([6]).to(device))\n",
        "IsCylinder = ltn.Predicate(FeatureModel([7]).to(device))\n",
        "IsCone = ltn.Predicate(FeatureModel([8]).to(device))\n",
        "IsTriangle = ltn.Predicate(FeatureModel([9]).to(device))\n",
        "\n",
        "# IsGreen: Usa FeatureModel para checar a cor verde (√≠ndice 3).\n",
        "IsGreen = ltn.Predicate(FeatureModel([3]).to(device))\n",
        "# IsSmall: Usa FeatureModel para checar o tamanho (√≠ndice 10).\n",
        "IsSmall = ltn.Predicate(FeatureModel([10]).to(device))\n",
        "\n",
        "# Predicados Bin√°rios (Rela√ß√µes entre dois objetos):\n",
        "# LeftOf, RightOf: Usam RelationModel para checar a rela√ß√£o no eixo X (√≠ndice 0).\n",
        "LeftOf = ltn.Predicate(RelationModel(0).to(device))\n",
        "RightOf = ltn.Predicate(RelationModel(0).to(device))\n",
        "# Below, Above: Usam RelationModel para checar a rela√ß√£o no eixo Y (√≠ndice 1).\n",
        "Below = ltn.Predicate(RelationModel(1).to(device))\n",
        "Above = ltn.Predicate(RelationModel(1).to(device))\n",
        "\n",
        "# CloseTo: Usa CloseToModel para verificar proximidade entre objetos.\n",
        "CloseTo = ltn.Predicate(CloseToModel().to(device))\n",
        "# SameSize: Usa SameSizeModel para verificar se objetos t√™m o mesmo tamanho.\n",
        "SameSize = ltn.Predicate(SameSizeModel().to(device))\n",
        "\n",
        "# --- OPERADORES L√ìGICOS FUZZY (Conectivos e Quantificadores) ---\n",
        "# LTN utiliza operadores l√≥gicos fuzzy para calcular a 'verdade' de proposi√ß√µes complexas.\n",
        "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
        "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
        "Or = ltn.Connective(ltn.fuzzy_ops.OrProbSum())\n",
        "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
        "# Quantificadores (Forall para \"para todo\", Exists para \"existe\")\n",
        "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
        "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
        "# Agregador para combinar os valores de verdade de m√∫ltiplos axiomas/regras.\n",
        "sat_agg = ltn.fuzzy_ops.AggregPMeanError(p=2)\n",
        "\n",
        "# --- FUN√á√ÉO PRINCIPAL DE EXECU√á√ÉO DO EXPERIMENTO ---\n",
        "# Esta fun√ß√£o encapsula um √∫nico ciclo de treinamento e avalia√ß√£o do LTN.\n",
        "def run_experiment(run_id):\n",
        "    print(f\"\\n--- INICIANDO EXECU√á√ÉO {run_id+1}/5 ---\")\n",
        "\n",
        "    # B. Gera√ß√£o de Dados e Vari√°veis LTN\n",
        "    # Usa o dataset de objetos detectados na imagem (ds_image) ao inv√©s de gerar dados sint√©ticos.\n",
        "    global ds_image # Declara ds_image como global para acessar o objeto criado na c√©lula 9.\n",
        "    data = ds_image.data # Obt√©m o tensor de dados dos objetos da imagem.\n",
        "\n",
        "    # Define vari√°veis LTN 'x', 'y' e 'z' sobre o conjunto de dados.\n",
        "    # LTN usa essas vari√°veis para quantificar sobre os objetos nos axiomas.\n",
        "    x = ltn.Variable(\"x\", data)\n",
        "    y = ltn.Variable(\"y\", data)\n",
        "    z = ltn.Variable(\"z\", data)\n",
        "\n",
        "    # C. Otimizador\n",
        "    # Otimizador Adam para ajustar os pesos das redes neurais dentro dos predicados.\n",
        "    # `list(Predicate.parameters())` coleta todos os par√¢metros trein√°veis dos modelos neurais.\n",
        "    optimizer = torch.optim.Adam(\n",
        "        list(IsCircle.parameters()) + list(IsSquare.parameters()) +\n",
        "        list(IsCylinder.parameters()) + list(IsCone.parameters()) + list(IsTriangle.parameters()) +\n",
        "        list(IsSmall.parameters()) + list(IsGreen.parameters()) +\n",
        "        list(LeftOf.parameters()) + list(RightOf.parameters()) +\n",
        "        list(Below.parameters()) + list(Above.parameters()), lr=0.01\n",
        "    )\n",
        "\n",
        "    # D. Loop de Treino\n",
        "    print(\"Treinando...\")\n",
        "    for epoch in range(300):\n",
        "        optimizer.zero_grad() # Zera os gradientes acumulados de itera√ß√µes anteriores.\n",
        "\n",
        "        # --- AXIONAS LTN ---\n",
        "        # Axiomas s√£o proposi√ß√µes l√≥gicas que o modelo deve aprender a satisfazer.\n",
        "        # O objetivo do treinamento √© maximizar o valor de verdade desses axiomas.\n",
        "\n",
        "        # 1. Axiomas de Taxonomia:\n",
        "        # sat_cov: Cobertura - Todo objeto deve ser uma das formas conhecidas (C√≠rculo OU Quadrado OU Cilindro OU Cone OU Tri√¢ngulo).\n",
        "        sat_cov = Forall(x, Or(Or(Or(Or(IsCircle(x), IsSquare(x)), IsCylinder(x)), IsCone(x)), IsTriangle(x)))\n",
        "        # sat_mut_1: Exclus√£o M√∫tua - Se √© um C√≠rculo, n√£o pode ser um Quadrado.\n",
        "        sat_mut_1 = Forall(x, Implies(IsCircle(x), Not(IsSquare(x))))\n",
        "        # sat_mut_2: Exclus√£o M√∫tua - Se √© um Quadrado, n√£o pode ser um Cilindro.\n",
        "        sat_mut_2 = Forall(x, Implies(IsSquare(x), Not(IsCylinder(x))))\n",
        "\n",
        "        # 2. Axiomas Espaciais (Horizontal):\n",
        "        # sat_lr_irref: Irreflexividade - Nenhum objeto pode estar √† sua pr√≥pria esquerda.\n",
        "        sat_lr_irref = Forall(x, Not(LeftOf(x, x)))\n",
        "        # sat_lr_inv: Invers√£o - Se X est√° √† esquerda de Y, ent√£o Y est√° √† direita de X.\n",
        "        sat_lr_inv = Forall([x, y], Implies(LeftOf(x, y), RightOf(y, x)))\n",
        "        # sat_lr_trans: Transitividade - Se X est√° √† esquerda de Y, e Y √† esquerda de Z, ent√£o X est√° √† esquerda de Z.\n",
        "        sat_lr_trans = Forall([x,y,z], Implies(And(LeftOf(x,y), LeftOf(y,z)), LeftOf(x,z)))\n",
        "\n",
        "        # 3. Axiomas Verticais:\n",
        "        # sat_ud_inv: Invers√£o - Se X est√° abaixo de Y, ent√£o Y est√° acima de X.\n",
        "        sat_ud_inv = Forall([x, y], Implies(Below(x, y), Above(y, x)))\n",
        "        # sat_ud_trans: Transitividade - Se X est√° abaixo de Y, e Y abaixo de Z, ent√£o X est√° abaixo de Z.\n",
        "        sat_ud_trans = Forall([x,y,z], Implies(And(Below(x,y), Below(y,z)), Below(x,z)))\n",
        "\n",
        "        # --- ADI√á√ÉO 1: REGRA DE PROXIMIDADE (Tarefa 3.3) ---\n",
        "        # Nova regra: \"Se dois objetos s√£o Tri√¢ngulos e est√£o Pr√≥ximos, devem ter o mesmo Tamanho\"\n",
        "        # Isso demonstra a capacidade do LTN de incorporar regras complexas.\n",
        "        sat_prox = Forall([x,y], Implies(And(IsTriangle(x), And(IsTriangle(y), CloseTo(x,y))), SameSize(x,y)))\n",
        "\n",
        "        # 4. Axiomas de Supervis√£o (Grounding):\n",
        "        # Estes axiomas supervisionam diretamente os predicados un√°rios com base nos dados brutos.\n",
        "        # Criam m√°scaras para identificar objetos de cada forma no conjunto de dados.\n",
        "        mask_circ = data[:, 5] == 1\n",
        "        mask_sq = data[:, 6] == 1\n",
        "        mask_cyl = data[:, 7] == 1\n",
        "        mask_cone = data[:, 8] == 1\n",
        "        mask_tri = data[:, 9] == 1\n",
        "\n",
        "        # Para cada forma, cria um axioma que diz: \"Para todo objeto de tipo X, ele √â X\".\n",
        "        # O `if mask_X.any() else ltn.Constant(torch.tensor(1.))` evita erros se n√£o houver objetos de uma certa forma.\n",
        "        s_circ = Forall(ltn.Variable(\"xc\", data[mask_circ]), IsCircle(ltn.Variable(\"xc\", data[mask_circ]))) if mask_circ.any() else ltn.Constant(torch.tensor(1.))\n",
        "        s_sq = Forall(ltn.Variable(\"xs\", data[mask_sq]), IsSquare(ltn.Variable(\"xs\", data[mask_sq]))) if mask_sq.any() else ltn.Constant(torch.tensor(1.))\n",
        "        s_cyl = Forall(ltn.Variable(\"xcy\", data[mask_cyl]), IsCylinder(ltn.Variable(\"xcy\", data[mask_cyl]))) if mask_cyl.any() else ltn.Constant(torch.tensor(1.))\n",
        "        s_cone = Forall(ltn.Variable(\"xco\", data[mask_cone]), IsCone(ltn.Variable(\"xco\", data[mask_cone]))) if mask_cone.any() else ltn.Constant(torch.tensor(1.))\n",
        "        s_tri = Forall(ltn.Variable(\"xt\", data[mask_tri]), IsTriangle(ltn.Variable(\"xt\", data[mask_tri]))) if mask_tri.any() else ltn.Constant(torch.tensor(1.))\n",
        "\n",
        "        # Agrega√ß√£o da Loss:\n",
        "        # Combina todos os valores de verdade dos axiomas usando `sat_agg`.\n",
        "        # O objetivo √© maximizar essa verdade agregada.\n",
        "        sat_total = sat_agg(torch.stack([\n",
        "            sat_cov.value, sat_mut_1.value, sat_mut_2.value,\n",
        "            sat_lr_irref.value, sat_lr_inv.value, sat_lr_trans.value,\n",
        "            sat_ud_inv.value, sat_ud_trans.value,\n",
        "            sat_prox.value,  # <--- Valor de verdade da nova regra de proximidade\n",
        "            s_circ.value, s_sq.value, s_cyl.value, s_cone.value, s_tri.value\n",
        "        ]))\n",
        "\n",
        "        # A fun√ß√£o de perda √© (1 - sat_total), ou seja, minimizar a perda √© maximizar a verdade.\n",
        "        loss = 1.0 - sat_total\n",
        "        loss.backward() # Calcula os gradientes.\n",
        "        optimizer.step() # Atualiza os pesos dos modelos neurais com base nos gradientes.\n",
        "\n",
        "    print(f\"Loss Final Run {run_id+1}: {loss.item():.4f}\")\n",
        "\n",
        "    # --- 5. AVALIA√á√ÉO E CONSULTAS FINAIS ---\n",
        "    # Ap√≥s o treinamento, formulamos e avaliamos consultas para verificar o conhecimento aprendido.\n",
        "\n",
        "    # q1: Existe um objeto pequeno (IsSmall) que esteja abaixo de um Cilindro (IsCylinder) E √† esquerda de um Quadrado (IsSquare)?\n",
        "    q1 = Exists(x, And(IsSmall(x), And(Exists(y, And(IsCylinder(y), Below(x,y))), Exists(z, And(IsSquare(z), LeftOf(x,z))))))\n",
        "    # q2: Existem 3 objetos (x, y, z) onde x √© um Cone verde (IsCone e IsGreen), y est√° √† esquerda de x, e x est√° √† esquerda de z?\n",
        "    q2 = Exists([x,y,z], And(And(IsCone(x), IsGreen(x)), And(LeftOf(y,x), LeftOf(x,z))))\n",
        "\n",
        "    # C. C√°lculo de M√©tricas de Classifica√ß√£o (para formas):\n",
        "    # Avalia a capacidade dos predicados de forma em classificar corretamente os objetos.\n",
        "    # true_shapes ser√° baseado nos dados da imagem, se objetos reais foram detectados.\n",
        "    true_shapes = torch.argmax(data[:, 5:10], dim=1).cpu().numpy()\n",
        "\n",
        "    p_s = []\n",
        "    for pred in [IsCircle, IsSquare, IsCylinder, IsCone, IsTriangle]:\n",
        "        p_s.append(pred.model(data).detach().cpu().numpy())\n",
        "    pred_shapes = np.argmax(np.stack(p_s, axis=1).squeeze(axis=2), axis=1)\n",
        "\n",
        "    acc = accuracy_score(true_shapes, pred_shapes)\n",
        "    prec = precision_score(true_shapes, pred_shapes, average='macro', zero_division=0)\n",
        "    rec = recall_score(true_shapes, pred_shapes, average='macro', zero_division=0)\n",
        "    f1 = f1_score(true_shapes, pred_shapes, average='macro', zero_division=0)\n",
        "\n",
        "    # --- ADI√á√ÉO 2: Retorno incluindo sat_prox (Necess√°rio para a tabela de resultados) ---\n",
        "    # Retorna os valores de verdade das consultas e da regra de proximidade,\n",
        "    # al√©m das m√©tricas de classifica√ß√£o.\n",
        "    return {\n",
        "        \"sat_q1\": q1.value.item(),\n",
        "        \"sat_q2\": q2.value.item(),\n",
        "        \"sat_regra_prox\": sat_prox.value.item(), # <--- Valor de verdade da regra de proximidade\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "# --- 6. EXECU√á√ÉO DOS 5 EXPERIMENTOS ---\n",
        "# Executa a fun√ß√£o `run_experiment` v√°rias vezes para obter resultados estatisticamente robustos.\n",
        "# Note que o treinamento ser√° repetido 5 vezes sobre os *mesmos* dados da imagem.\n",
        "results = []\n",
        "for i in range(5):\n",
        "    res = run_experiment(i)\n",
        "    results.append(res)\n",
        "\n",
        "# --- 7. RELAT√ìRIO FINAL ---\n",
        "# Calcula e imprime a m√©dia e o desvio padr√£o dos resultados de todas as execu√ß√µes.\n",
        "print(\"\\n=== RESULTADOS FINAIS (M√âDIA DE 5 EXECU√á√ïES) ===\")\n",
        "keys = results[0].keys()\n",
        "for k in keys:\n",
        "    values = [r[k] for r in results]\n",
        "    mean_val = np.mean(values)\n",
        "    std_val = np.std(values)\n",
        "    print(f\"{k}: {mean_val:.4f} (+/- {std_val:.4f})\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
