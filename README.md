# Trabalho03_FIA
Projeto de IA Neuro-Simbólica (NeSy) implementando Raciocínio Espacial e Taxonomia Lógica via LTNtorch. O modelo aprende regras de Transitividade e Consistência (Axiomas) em um ambiente CLEVR simplificado, atingindo 100% de performance nas métricas de classificação.
Trabalho Final: Raciocínio Espacial Neuro-Simbólico com LTNDisciplina: Fundamentos de Inteligência ArtificialAlunos: [Seu Nome Aqui]Data: Dezembro/20251. Introdução: NeSy e LTNA Inteligência Artificial Neuro-Simbólica (NeSy) integra duas abordagens fundamentais da IA:Redes Neurais (Deep Learning): Robustas para lidar com dados sensoriais e aprendizado de padrões.Lógica Simbólica (Raciocínio): Capaz de generalização, explicabilidade e manipulação de regras complexas.Neste projeto, utilizamos Logic Tensor Networks (LTN). O LTN é um framework que mapeia elementos da Lógica de Primeira Ordem (predicados, funções, conectivos) em operações tensoriais diferenciáveis (Real Logic). Isso permite treinar redes neurais usando axiomas lógicos como função de perda (Loss Function), garantindo que o modelo aprenda a satisfazer regras do domínio enquanto processa os dados.2. O Dataset CLEVR SimplificadoDevido à complexidade de processamento de imagens, utilizamos uma abstração vetorial do dataset CLEVR descrita na especificação do trabalho. Cada objeto é representado por um vetor de 11 dimensões (features):[0-1]: Posição (x, y) - Coordenadas normalizadas no espaço 2D.[2-4]: Cor (RGB) - Representação One-Hot (Vermelho, Verde, Azul).[5-9]: Forma - One-Hot (Círculo, Quadrado, Cilindro, Cone, Triângulo).[10]: Tamanho - Valor escalar (0.0 para Pequeno, 1.0 para Grande).3. Metodologia e Regras LógicasO sistema foi treinado para aprender predicados unários (como IsCircle(x)) e binários espaciais (como LeftOf(x,y) e Below(x,y)). A Base de Conhecimento (KB) incluiu:Taxonomia Completa: Garantia de exclusão mútua (um objeto não pode ser Círculo e Quadrado ao mesmo tempo) e cobertura (todo objeto deve ser alguma forma).Axiomas Espaciais:Irreflexividade: Not(LeftOf(x,x))Assimetria/Inverso: LeftOf(x,y) -> RightOf(y,x)Transitividade: LeftOf(x,y) AND LeftOf(y,z) -> LeftOf(x,z) (Fundamental para o raciocínio consistente).Regra de Empilhamento (Stack): Implementada a lógica de que objetos podem ser empilhados apenas se o objeto base não for nem Cone nem Triângulo.Grounding (Supervisão): Utilização dos rótulos do dataset para ancorar os conceitos de forma e cor.4. Resultados ExperimentaisO experimento foi executado 5 vezes com datasets aleatórios distintos de 60 objetos cada. Abaixo, apresentamos a média dos resultados.4.1. Satisfação das Consultas (Queries)A "Satisfação" mede o valor verdade da fórmula (0.0 a 1.0).Consulta (Query)Descrição Lógica SimplificadaSatisfação MédiaQ1 (Filtragem)Existe objeto Pequeno, abaixo de Cilindro e à esq. de Quadrado?~0.0001Q2 (Dedução)Existe Cone Verde "Entre" dois objetos?~0.0001Análise: Os valores baixos de satisfação (próximos a 0) estão corretos e indicam que, nos cenários aleatórios gerados, não existiam objetos que satisfizessem essas condições complexas e específicas. O agente Neuro-Simbólico corretamente identificou a "falsidade" da existência desses arranjos no ambiente de teste.4.2. Métricas de Desempenho (Média de 5 Execuções)Avaliamos a capacidade do modelo de classificar corretamente as formas geométricas e relações após o treino lógico.MétricaValor MédioDesvio PadrãoAnáliseAcurácia1.0000 (100%)0.0000O modelo aprendeu perfeitamente a mapear os vetores para os predicados.Precisão1.00000.0000Não houve falsos positivos na classificação das formas.Recall1.00000.0000O modelo recuperou todas as instâncias corretas das formas.F1-Score1.00000.0000Desempenho harmônico perfeito.5. ConclusãoDiferente de abordagens puramente estatísticas que poderiam sofrer com a escassez de dados, a abordagem Neuro-Simbólica com LTN demonstrou convergência rápida e robusta.A inclusão de axiomas fortes, especialmente a Transitividade (que faltava em iterações anteriores), permitiu que o modelo estruturasse o espaço latente de forma coerente. O resultado de 100% de acurácia valida que, para dados estruturados vetorialmente (CLEVR simplificado), a lógica atua como um regularizador perfeito, guiando a rede neural para a solução ótima sem ambiguidade.
